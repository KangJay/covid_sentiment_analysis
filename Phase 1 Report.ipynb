{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "further-chemistry",
   "metadata": {},
   "source": [
    "# Utilizing Twitter to Measure the Public's Sentiment on the Pandemic & Its Relevant Protective Measures Over Time\n",
    "**Team 7:** Insert Name Here \n",
    "\n",
    "**Members:** Lipsa J., Ji K., Yuanfeng L., Yu L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-moscow",
   "metadata": {},
   "source": [
    "### Background and Motivation\n",
    "The pandemic has uprooted the lives of every single person in the world. While it began as a minor inconvenience to many people, the harsh reality and severity of the virus were soon realized. In the beginning of enforcing protective measures to protect the public, many people's opinions on the virus, protective rules & procedures, and other topics relating to the pandemic have changed and continually do into 2021. \n",
    "\n",
    "With such a slow response to protective measures in the U.S compared to other countries globally, we wanted to find out the public's stance on the matter over the period of nearly the entire pandemic. \n",
    "\n",
    "With this in mind, we want to record and analyze these trends by looking at the metrics such as sentiment, LIWC metrics, and possibly more as we make further discoveries.\n",
    "\n",
    "Utilizing Twitter, an online social media platform for sharing content and microblogging, we'll be analyzing \"tweets\" (publically posted messages) from everyday people about how they feel about the pandemic. This procedure will be run on data from January 22 2020, all the way to the most current data being available at this current time of the project (February and March of 2021). We believe this approach will work compared to traditional forms of collecting data on the topic. Twitter specifically has proven itself to be accurate, quick, and better reflect the perspectives of the everyday person since they're the ones whose data we're processing.\n",
    "\n",
    "We've outlined objectives & research questions we hope to answer through this approach.\n",
    "* **Goal 1**: Find out how many tweets sentiments changed on the regulation or rules about wearing a mask or taking a vaccine for the the year 2020 and current months in 2021 (January - March)\n",
    "* **Goal 2**: Find out the sentiment of tweets relating to the COVID-19 virus for the year 2020 and the current months in 2021 (January - March)\n",
    "* **Stretch Goal 1**: Find out the sentiments across geographical locations within the U.S about either protective measures (Eg. Wearing a mask) and the taking the vaccine. It's been shown throughout various news outlets and social media that different areas in the U.S have had varying responses to these rules. If time & resources allow, we want to run the research experiment at a lower level - focusing on specific areas in the U.S - Perhaps areas with the lowest cases per capita vs. moderate vs. high. \n",
    "* **Stretch goal 2**: Relate our findings to how misinformation & fake news on Twitter changed before and after the election; as well as its possible consequences on the public's sentiment on the topic of COVID-19 and its related topics (Eg. vaccines, lockdown, social distancing).\n",
    "\n",
    "Through our efforts, we hope to be able to answer or at least find insight into the following questions as well:\n",
    "* Have specific events affected the public's stance on the pandemic? These could be the presidential election, the presidential candidates debate as well as the vice-presidents debate, Trump getting diagnosed and hospital stayed, and so on. \n",
    "* How have different cities, counties, and states efficacy in containing the virus relate to the public sentiment from the people there?\n",
    "* Is there a positive trend or at least an upward direction for sentiment on social media comparing pre-election to post-election? \n",
    "* What are the trends in Democratic-heavy vs. Republican-heavy vs. Well-mixed (Eg. close to 50/50 Democrat to Republican) States in terms of the pandemic and protective measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-bunch",
   "metadata": {},
   "source": [
    "### Our Current Approach\n",
    "Currently, we're utilizing Twitter for our data collection. Initially, we stated that we'd be utilizing Reddit as well but the caveats of a public forum platform is that it's heavily moderated. With the pandemic being a global crisis, Reddit has emphasized initiatives to remove and censor posts that may be incediary, controversial, promote misinformation and so on. While these things are negative in the grand scheme of society, we actually want to collect this kind of data as well since it shows a sub-population with different views. There is a promising subreddit about Parler, the right-wing social media platform, in which users post the most outrageous or controversial posts they see on the platform to re-post and discuss on reddit. This seemed promising at first but the format mirrors satire and other users have cherry-picked those specific posts and typically upload them as images. It wouldn't accurately reflect the Parler population on specific topics, would be difficult to parse for specific keywords, and may not produce enough data.\n",
    "\n",
    "We're utilizing Tweepy which is Twitter's API wrapper for Python. It's extremely easy to utilize but one of its caveats is that it will only look at the past week to pull data; which makes sense since many people actually use real-time data for analysis. To get around this issue, we relied on Kaggle and IEEE.\n",
    "Both of them have been data mining the ID number of tweets with keywords relating to the pandemic since near the beginning of 2020. These keywords include identifiers such as \"n95\", \"ppe\", \"washyourhands\", \"stayathome\", \"selfisolating\", \"social distancing\", \"covid-19\", and so on. \n",
    "\n",
    "Utilizing Tweepy and Python, we iterate through these tweet id values to pull the actual tweet status object from Twitter. From there, we extract the following information: \n",
    "* id: ID number of the tweet\n",
    "* username: Username of the person who posted the tweet\n",
    "* text: The literal text content of the tweet\n",
    "* entities: Hashtags the tweet had\n",
    "* retweet_count: Number of times the tweet had been retweeted\n",
    "* favorite_count: Number of times the tweet had been favorited\n",
    "* created_at: Time the tweet had been posted\n",
    "\n",
    "We're collecting our own data currently with the same parameters and keywords. Taking these datasets, in a csv format, we're running each through LIWC and looking at the following metrics: \n",
    "* Summary variables: Analytical thinking, clout, authentic, and emotional tone\n",
    "* Affect words: Positive emotions, negative emotions, anxiety, anger, sadness\n",
    "* Social words: Family, friends, female referents, male referents\n",
    "* Cognitive Processes: Insight\n",
    "* Biological processes: Body, health/illness\n",
    "* Personal concerns: Work, leisure, money\n",
    "* Informal speech: Swear words\n",
    "\n",
    "While the biggest contributors will be relating to authenticity, emotions (emotional tone & positive/negative emotions), we believe the other attributes will aid answering in our research questions and stretch goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-metadata",
   "metadata": {},
   "source": [
    "## Collecting Twitter Data From the Entirety of 2020\n",
    "As mentioned previously, since we can only directly scrape tweets for the past week, we utilize Kaggle's dataset which is found at https://www.kaggle.com/lopezbec/covid19-tweets-dataset.\n",
    "\n",
    "Additionally, the IEEE have published a similar dataset with a wider range of keywords which can be found at https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset. \n",
    "\n",
    "These files contain minimal data in order to save space. Kaggle's has just the tweet ids in a list-like structure, while the IEEE has a similar format but in pairs of tweet id and a sentiment score calculated for the content of the tweet. \n",
    "\n",
    "Here are a sample of what the Kaggle files look like. We'll be including sized down versions of them in our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-murray",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "kaggle_sample = open('./sample_data/sample_kaggle.txt', 'r')\n",
    "#removes beginning and ending '[' and  ']'\n",
    "content = kaggle_sample.read()[1:-1] \n",
    "#Delimit on comma, convert to int, store into a numpy array for parsing\n",
    "ids = np.fromstring(content, dtype=int, sep= ',')\n",
    "print(\"Number of tweets in sample: {}\".format(len(ids)))\n",
    "print(\"Sample of the Tweet IDS: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-sculpture",
   "metadata": {},
   "source": [
    "As mentioned, to save space Kaggle stores these tweet ids as a plaintext, in a list like structure... Eg. [id1, id2, id3, ...., idn]\n",
    "\n",
    "On the otherhand, IEEE provides theirs as a csv format or a zipped file containing a csv file (to save space) and theirs looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-sherman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ieee_sample = pd.read_csv(\"./sample_data/sample_ieee.csv\", header=None)\n",
    "ieee_sample.columns = ['tweet_id', 'sentiment']\n",
    "ieee_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-knowing",
   "metadata": {},
   "source": [
    "The IEEE data is much easier to work with since we can extract the tweet_id values directly by extracting the column. However, both are eventually in the same format which lets us run our data collector.\n",
    "It should be noted that, unlike the Tweepy API's 7 day period that it can return data for, the get_status() method for getting individual tweets given a tweet id does each call one by one. This takes much longer time but allows us to still get 15,000 records under the rate limit which means we collect data as fast as Tweepy will let us regardless.\n",
    "\n",
    "## Processing the Tweet IDs\n",
    "Due to the organizational structure both IEEE and Kaggle have (Eg. each day have their own files, files are grouped by month, etc.), we followed a similar approach. Below is code taken from a previous personal project Ji-Hoon has done just to iterate through directories. The directory navigation portion isn't entirely important but for breadth and covering bases, we've included this portion if you want to run the file as well.\n",
    "\n",
    "To perserve the name and directory structure each sub-directory has, it is a recursive method that holds the absolute path of the files (So Python can read in the file's contents) while also preserving the subdirectory path so we can name each file accordingly and know which dataset it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-chapter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "\n",
    "max_num_records = 5000\n",
    "\n",
    "\"\"\"\n",
    "Methodology we covered in class to just load the twitter credentials into appropriate objects.\n",
    "This assumes a file with your Twitter developer credentials are in a file named 'twitter.json' \n",
    "and is in the same directory as the program when it's being run.\n",
    "\"\"\"\n",
    "def load_keys(key_file):\n",
    "    with open(key_file) as f:\n",
    "        key_dict = json.load(f)\n",
    "    return key_dict['api_key'], key_dict['api_secret'], key_dict['token'], key_dict['token_secret']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Recursive method to navigate through many directories.\n",
    "\"\"\"\n",
    "def iterate_files(path, subdir):\n",
    "    KEY_FILE = \"./twitter.json\" # Twitter credentials. See def load_keys(key_file):\n",
    "    api_key, api_secret, token, token_secret = load_keys(KEY_FILE)\n",
    "    auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "    auth.set_access_token(token, token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    # File recursion portion. Not pertinent to the actual data collection.\n",
    "    for filename in os.listdir(path):\n",
    "        filePath = path + \"/\" + filename\n",
    "        # If folder, recursively call.\n",
    "        if filename == 'scraped_data': # Skip folder containing the scraped data.\n",
    "            continue\n",
    "        if (os.path.isdir(filePath)):\n",
    "            tempSubdir = \"\"\n",
    "            if subdir: tempSubdir = subdir + \"/\" + filename\n",
    "            else: tempSubdir = filename\n",
    "            iterate_files(filePath, tempSubdir)\n",
    "        # Otherwise, process the file.\n",
    "        else:\n",
    "            filekey = subdir\n",
    "            if subdir: file = subdir + \"/\" + filename\n",
    "            else: file = filename\n",
    "            tweet_content = defaultdict(list)\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO: Refactor to detect .txt and .csv to know which dataset it came from. \n",
    "            \"\"\"\n",
    "            # So it doesn't read itself or the credentials file\n",
    "            #if filename not in ['process_ieee.py', 'process_tweets.py', 'twitter.json']: \n",
    "            if filename.endswith('.csv'):\n",
    "                # NOTE: This version of the program is assuming .csv files -- IEEE data. \n",
    "                # Samples 5000 records from the data set, takes only the column values, then ravels into a np array\n",
    "                tweet_ids = pd.read_csv(filePath).sample(n=max_num_records).iloc[:,0].values.ravel()\n",
    "                print(\"Collecting from file: {}\".format(filename))\n",
    "                # Iterates through each of the tweet ids we sampled.\n",
    "                for id in tweet_ids: \n",
    "                    \"\"\"\n",
    "                    Must be in a try-catch structure. If a twitter user is banned or suspended, the tweet_id\n",
    "                    refers to data that doesn't exist. The Tweepy api will return a 400-level HTTP status code\n",
    "                    due to the resource not being found - which is considered an exception.\n",
    "                    \"\"\"\n",
    "                    try: \n",
    "                        print(num_records) # Debugging. Just to see that the program wasn't stalling.\n",
    "                        tweet = api.get_status(id) #returns status object\n",
    "                    except tweepy.RateLimitError:\n",
    "                        print(\"Rate Limit hit. Sleeping for 15 minutes.\")\n",
    "                        time.sleep(900)\n",
    "                        print(\"Resuming...\\n\")\n",
    "                        continue\n",
    "                    except Exception as e: # It will throw an exception if twitter user has actually been suspended\n",
    "                        continue\n",
    "                    if tweet is None:\n",
    "                        print(\"Should never be reached. If seen, something went wrong.\")\n",
    "                    \n",
    "                    # Features we're extracting.\n",
    "                    tweet_content['id'].append(tweet.id)\n",
    "                    tweet_content['username'].append(tweet.user.name)\n",
    "                    tweet_content['text'].append(tweet.text)\n",
    "                    tweet_content['entities'].append(tweet.entities)\n",
    "                    tweet_content['retweet_count'].append(tweet.retweet_count)\n",
    "                    tweet_content['favorite_count'].append(tweet.favorite_count)\n",
    "                    tweet_content['created_at'].append(tweet.created_at)\n",
    "                result_filename = './scraped_data/' + filename\n",
    "                \"\"\"\n",
    "                We control how many records we want from each day. It'll either run through the entire file\n",
    "                or run based on how many records we want sampled.\n",
    "                \"\"\"\n",
    "                pd.DataFrame(tweet_content).to_csv(result_filename)\n",
    "                print(\"Done processing: {}\".format(result_filename))\n",
    "\n",
    "\n",
    "def get_path():\n",
    "    iterate_files(os.getcwd(), \"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-member",
   "metadata": {},
   "source": [
    "An important note to make here is that this Python code wasn't designed to run in a Jupyter Notebook. If you wish to replicate this portion, place this python program in the same directory as the IEEE sample tweet ids along with your twitter.json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-fluid",
   "metadata": {},
   "source": [
    "## Data Collection For Recent Data in 2021\n",
    "We relied on those datasets to help supplement the earlier twitter data we can't directly retrieve. However, for the current data, we're querying and data mining with a similar approach. \n",
    "\n",
    "We have taken inspiration from the IEEE Coronavirus (COVID-19) Tweets Dataset, which can be found at https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset. They have collected tweets relating to a large set of keywords since the very beginning of the pandemic and continually do so. We have taken a scaled down version and taken specific keywords from their larger set - which can be found at https://rlamsal.com.np/keywords.tsv. Note: The link will start the download of a tab-separated file with the keywords but is small in terms of memory size. Just a warning.\n",
    "\n",
    "The approach for collecting recent data has stayed essentially the same as we did in our Lab 3 for the introduction to Tweepy. One difference is that, since we're all constrained with the rate limit, we've separated the main topics to be scraping for. The keywords each person utilized to scrape will be shown at the credits/work distribution section at the end.\n",
    "\n",
    "Dividing up the number of keywords, we each scraped data looking for our specific delegated keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the keywords that I would like to search on\n",
    "\"\"\"\n",
    "NOTE: This is assuming that the twitter credentials were loaded properly\n",
    "prior to being run.\n",
    "\n",
    "This version of code is looking specifically at tweets relating to masks.\n",
    "Other keywords scraped are shown in the work distribution/credits.\n",
    "\"\"\"\n",
    "keywords = [\"#Wearmask\", \n",
    "            \"#Wearmasks\",\n",
    "            \"#mask\",\n",
    "            \"#masks4all\",\n",
    "            \"#n95 mask\",\n",
    "            \"#n95 respirator mask\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "SLEEP_TIME = 60 * 15\n",
    "\n",
    "\n",
    "def read_write_tweets(search_term, target_page_list):\n",
    "    \"\"\"\n",
    "    it search the serach term in twitter, and write num_items items \n",
    "    in csv_file\n",
    "    \n",
    "    @parameters:\n",
    "    search_term: the keyword you use to search, such as 'covid19'\n",
    "    num_items: how many items for each search\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    page_list = target_page_list\n",
    "    try:\n",
    "        for page in tweepy.Cursor(api.search, q=search_term + \" -filter:retweets\", lang\n",
    "                                  = 'en', tweet_mode=\"extended\").pages():\n",
    "            if page not in page_list:\n",
    "                page_list.append(page)\n",
    "                for tweet in page:\n",
    "                    csvWriter.writerow([tweet.id, tweet.user.name, tweet.full_text, tweet.entities, tweet.retweet_count,\n",
    "                                    tweet.favorite_count, tweet.created_at])\n",
    "    # Rate limit hit. Must sleep for 15 minutes.\n",
    "    except Exception as e:\n",
    "        print(e, '; Will Sleep for:',SLEEP_TIME)\n",
    "        print(\"now time:\", datetime.datetime.now().time())\n",
    "        # Print out current version of data scraped\n",
    "        return_df_info(filename)\n",
    "        # Sleep for 15 minutes\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        # Resume scraping\n",
    "        read_write_tweets(search_term, page_list)\n",
    "\n",
    "# make filename, \n",
    "filename = 'ProjectPhase_1_Fv05_' + (datetime.datetime.now().strftime(\"%Y-%m-%d-%H\")) + '.csv'\n",
    "# r+ will not created the file, if it not existed, rest the same\n",
    "\n",
    "import datetime\n",
    "\n",
    "time_start = datetime.datetime.now().time()\n",
    "print(\"Starting at:\", time_start)\n",
    "with open (filename, 'a+', newline='', encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    # first row is the titles for columns\n",
    "    csvWriter.writerow([\"tweet_id\", \"username\", \"text_of_tweet\", \"tweet_entities\", \"retweet_count\", \"favorite_count\", \"created_at\"])\n",
    "    # for each keyword, write 350 items in csv_file\n",
    "    for keyword in keywords: \n",
    "        read_write_tweets(keyword,[])\n",
    "\n",
    "\n",
    "time_finish = datetime.datetime.now().time()\n",
    "print(\"Finished at:\", time_finish)\n",
    "print(\"Spendt time:\", datetime.datetime.combine(datetime.date.today(), time_finish) - datetime.datetime.combine(datetime.date.today(), time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-bread",
   "metadata": {},
   "source": [
    "Since we're all searching for different keywords, it may show that we have duplicates in our set. So we have defined a simple way to see data as we aggregate our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "included-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3388, 101)\n",
      "duplicated rows: 0\n",
      "not duplicated data: 3388\n",
      "df head(): \n",
      "    Source (A)           Source (B)       Source (C)  \\\n",
      "0         NaN                   id         username   \n",
      "1         0.0  1245016268060622849           angel🦋   \n",
      "2         1.0  1245147924289503232              Rui   \n",
      "3         2.0  1244894426872324096     Zeno Protect   \n",
      "4         3.0  1244868044498841605  destiny's niece   \n",
      "\n",
      "                                          Source (D)  \\\n",
      "0                                               text   \n",
      "1       RT @DailyDoseOfKia: IF “IDGAF” Was A State 😭   \n",
      "2                                        Corona time   \n",
      "3  ⚠️✋🏻Schools are empty due to the Corona Crises...   \n",
      "4  i thought ansel elgort got corona bc he was tr...   \n",
      "\n",
      "                                          Source (E)     Source (F)  \\\n",
      "0                                           entities  retweet_count   \n",
      "1  {'hashtags': [], 'symbols': [], 'user_mentions...           4830   \n",
      "2  {'hashtags': [], 'symbols': [], 'user_mentions...              0   \n",
      "3  {'hashtags': [], 'symbols': [], 'user_mentions...              0   \n",
      "4  {'hashtags': [], 'symbols': [], 'user_mentions...              0   \n",
      "\n",
      "       Source (G)           Source (H)  WC  Analytic  ...  Comma  Colon  \\\n",
      "0  favorite_count           created_at   1     93.26  ...    0.0   0.00   \n",
      "1               0  2020-03-31 15:53:04   7     29.30  ...    0.0  14.29   \n",
      "2               1  2020-04-01 00:36:13   2     93.26  ...    0.0   0.00   \n",
      "3               0  2020-03-31 07:48:55  25     99.00  ...    0.0   0.00   \n",
      "4               2  2020-03-31 06:04:04  18      4.69  ...    0.0   0.00   \n",
      "\n",
      "   SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
      "0    0.0    0.0     0.0   0.0   0.00      0.0      0.0    0.00  \n",
      "1    0.0    0.0     0.0   0.0  28.57      0.0      0.0   14.29  \n",
      "2    0.0    0.0     0.0   0.0   0.00      0.0      0.0    0.00  \n",
      "3    0.0    0.0     0.0   0.0   0.00      0.0      0.0    8.00  \n",
      "4    0.0    0.0     0.0   0.0   0.00      0.0      0.0    0.00  \n",
      "\n",
      "[5 rows x 101 columns]\n",
      "df tail(): \n",
      "       Source (A)           Source (B)             Source (C)  \\\n",
      "3383      3382.0  1244896269539016705                  Amaan   \n",
      "3384      3383.0  1244946151020933122  Engr. H A Waheed Butt   \n",
      "3385      3384.0  1244900194598051840           Rezaul haque   \n",
      "3386      3385.0  1245090887555678208  dobrik / datalie stan   \n",
      "3387      3386.0  1244905249128644614            Sarath Nair   \n",
      "\n",
      "                                             Source (D)  \\\n",
      "3383  RT @LOLrakshak: An Indian virologist has devel...   \n",
      "3384  RT @kdastgirkhan: \"Keys to resolve the Corona ...   \n",
      "3385  RT @AunindyoC: Religious congregation on March...   \n",
      "3386  sad that joe and annelise have the corona. i r...   \n",
      "3387  RT @oommen: @sushantsareen \"All the corona pat...   \n",
      "\n",
      "                                             Source (E) Source (F) Source (G)  \\\n",
      "3383  {'hashtags': [], 'symbols': [], 'user_mentions...       1327          0   \n",
      "3384  {'hashtags': [], 'symbols': [], 'user_mentions...         38          0   \n",
      "3385  {'hashtags': [], 'symbols': [], 'user_mentions...        589          0   \n",
      "3386  {'hashtags': [], 'symbols': [], 'user_mentions...          0          0   \n",
      "3387  {'hashtags': [], 'symbols': [], 'user_mentions...         14          0   \n",
      "\n",
      "               Source (H)  WC  Analytic  ...  Comma  Colon  SemiC  QMark  \\\n",
      "3383  2020-03-31 07:56:14  25     63.83  ...   4.00   4.00    0.0   0.00   \n",
      "3384  2020-03-31 11:14:27  21     99.00  ...   9.52   9.52    0.0   0.00   \n",
      "3385  2020-03-31 08:11:50  21     93.26  ...   4.76   4.76    0.0   4.76   \n",
      "3386  2020-03-31 20:49:34  14     29.30  ...   0.00   0.00    0.0   0.00   \n",
      "3387  2020-03-31 08:31:55  25     30.73  ...   0.00   4.00    0.0   0.00   \n",
      "\n",
      "      Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
      "3383     0.0   0.0   0.00      0.0      0.0    4.00  \n",
      "3384     0.0   0.0   4.76      0.0      0.0    4.76  \n",
      "3385     0.0   0.0   0.00      0.0      0.0    4.76  \n",
      "3386     0.0   0.0   0.00      0.0      0.0    0.00  \n",
      "3387     0.0   0.0   8.00      0.0      0.0    8.00  \n",
      "\n",
      "[5 rows x 101 columns]\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing informations\n",
    "import pandas as pd\n",
    "\n",
    "def return_df_info(target_file):\n",
    "    df = pd.read_csv(target_file)\n",
    "    print(\"shape:\", df.shape)\n",
    "    dup = df.duplicated().sum()\n",
    "    print(\"duplicated rows:\", dup)\n",
    "    print(\"not duplicated data:\", df.shape[0] - dup)\n",
    "    print(\"df head(): \\n\", df.head())\n",
    "    print(\"df tail(): \\n\", df.tail())\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "# return_df_info(\"ProjectPhase_test012021-02-20-13.csv\")\n",
    "\n",
    "return_df_info(\"./scraped_data/LIWC2015 Results (march_31_2020).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-assets",
   "metadata": {},
   "source": [
    "## Processing the Data\n",
    "We collected all the data in the same fashion, ran the text fields through LIWC, and separated files to organize based on the time period they represent. We ran these files through LIWC's program which outputs a copy of the same file but with new columns pertaining to the metrics and corresponding values LIWC has produced.\n",
    "\n",
    "A snippet of the outputted file can be shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "racial-comparative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   standard            id            username  \\\n",
      "0         0  1.363275e+18        Fire Is Born   \n",
      "1         1  1.363263e+18   MyFrenchDietitian   \n",
      "2         2  1.363260e+18  healingcolorsmusic   \n",
      "3         3  1.363260e+18  healingcolorsmusic   \n",
      "4         4  1.363260e+18  healingcolorsmusic   \n",
      "\n",
      "                                                text  \\\n",
      "0  @iamungit I've been in one of them in San Fran...   \n",
      "1  Enjoy the #weekend, go #outdoor, reconnect wit...   \n",
      "2  #healingcolorsmusic #art #music ...there is #S...   \n",
      "3  #healingcolorsmusic #art #music ...there is #S...   \n",
      "4  #healingcolorsmusic #art #music ...there is #S...   \n",
      "\n",
      "                                            entities retweet_count  \\\n",
      "0  {'hashtags': [{'text': 'WearMask', 'indices': ...             1   \n",
      "1  {'hashtags': [{'text': 'weekend', 'indices': [...             0   \n",
      "2  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "3  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "4  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "\n",
      "  favorite_count           created_at  WC Analytic  ... Quote Apostro Parenth  \\\n",
      "0              1  2021-02-20 23:52:00  25    93.26  ...   0.0     4.0     0.0   \n",
      "1              1  2021-02-20 23:01:59  24    93.26  ...   0.0     0.0     0.0   \n",
      "2              1  2021-02-20 22:52:27  19    93.26  ...   0.0     0.0     0.0   \n",
      "3              1  2021-02-20 22:50:59  19    93.26  ...   0.0     0.0     0.0   \n",
      "4              1  2021-02-20 22:49:58  19    93.26  ...   0.0     0.0     0.0   \n",
      "\n",
      "  OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
      "0  16.00           NaN           NaN           NaN           NaN   \n",
      "1  29.17           NaN           NaN           NaN           NaN   \n",
      "2  52.63           NaN           NaN           NaN           NaN   \n",
      "3  52.63           NaN           NaN           NaN           NaN   \n",
      "4  52.63           NaN           NaN           NaN           NaN   \n",
      "\n",
      "   Unnamed: 105  Unnamed: 106  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 107 columns]\n",
      "Column Names: ['standard', 'id', 'username', 'text', 'entities', 'retweet_count', 'favorite_count', 'created_at', 'WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP', 'Unnamed: 101', 'Unnamed: 102', 'Unnamed: 103', 'Unnamed: 104', 'Unnamed: 105', 'Unnamed: 106']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihk/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (10,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_csv('./scraped_data/LIWC2015_feb.csv')\n",
    "print(sample.head())\n",
    "\n",
    "print(\"Column Names: {}\".format(list(sample.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-canada",
   "metadata": {},
   "source": [
    "We have noticed some problems across different operating systems for handling csv files. We initially ran into problems while sharing datasets with each other across Debian, Windows, and Mac and have resolved most of them since. One example is that some empty columns will show themselves as \"Unnamed\" columns with empty or NaN values in them. We ignore these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dress-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>40342</td>\n",
       "      <td>1.363335e+18</td>\n",
       "      <td>King Jamison Fawkes ♚</td>\n",
       "      <td>@WildHogPower \"WELL WELL WELL WELL WELL WELL W...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 03:50:29</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>43474</td>\n",
       "      <td>1.363335e+18</td>\n",
       "      <td>King Jamison Fawkes ♚</td>\n",
       "      <td>@WildHogPower \"WELL WELL WELL WELL WELL WELL W...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-21 03:50:29</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9187</th>\n",
       "      <td>21521</td>\n",
       "      <td>1.363309e+18</td>\n",
       "      <td>FA_eye(formally Accureye) #CyberPunk2077</td>\n",
       "      <td>@TheSphereHunter Nice love it great mask</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 02:05:49</td>\n",
       "      <td>6</td>\n",
       "      <td>62.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12488</th>\n",
       "      <td>32070</td>\n",
       "      <td>1.363325e+18</td>\n",
       "      <td>TEA POt</td>\n",
       "      <td>@FailedSoul_ *winning laughs* pretty impressiv...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 03:10:45</td>\n",
       "      <td>8</td>\n",
       "      <td>72.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14506</th>\n",
       "      <td>38373</td>\n",
       "      <td>1.363325e+18</td>\n",
       "      <td>TEA POt</td>\n",
       "      <td>@FailedSoul_ *winning laughs* pretty impressiv...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-21 03:10:45</td>\n",
       "      <td>8</td>\n",
       "      <td>72.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       standard            id                                  username  \\\n",
       "15037     40342  1.363335e+18                     King Jamison Fawkes ♚   \n",
       "15994     43474  1.363335e+18                     King Jamison Fawkes ♚   \n",
       "9187      21521  1.363309e+18  FA_eye(formally Accureye) #CyberPunk2077   \n",
       "12488     32070  1.363325e+18                                   TEA POt   \n",
       "14506     38373  1.363325e+18                                   TEA POt   \n",
       "\n",
       "                                                    text  \\\n",
       "15037  @WildHogPower \"WELL WELL WELL WELL WELL WELL W...   \n",
       "15994  @WildHogPower \"WELL WELL WELL WELL WELL WELL W...   \n",
       "9187            @TheSphereHunter Nice love it great mask   \n",
       "12488  @FailedSoul_ *winning laughs* pretty impressiv...   \n",
       "14506  @FailedSoul_ *winning laughs* pretty impressiv...   \n",
       "\n",
       "                                                entities retweet_count  \\\n",
       "15037  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "15994  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "9187   {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "12488  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "14506  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "\n",
       "      favorite_count           created_at  WC Analytic  ... Quote Apostro  \\\n",
       "15037              0  2021-02-21 03:50:29  26        1  ...  3.85     0.0   \n",
       "15994              1  2021-02-21 03:50:29  26        1  ...  3.85     0.0   \n",
       "9187               0  2021-02-21 02:05:49   6    62.04  ...  0.00     0.0   \n",
       "12488              0  2021-02-21 03:10:45   8    72.69  ...  0.00     0.0   \n",
       "14506              1  2021-02-21 03:10:45   8    72.69  ...  0.00     0.0   \n",
       "\n",
       "      Parenth OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
       "15037     0.0  11.54           NaN           NaN           NaN           NaN   \n",
       "15994     0.0  11.54           NaN           NaN           NaN           NaN   \n",
       "9187      0.0  16.67           NaN           NaN           NaN           NaN   \n",
       "12488     0.0  50.00           NaN           NaN           NaN           NaN   \n",
       "14506     0.0  50.00           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       Unnamed: 105  Unnamed: 106  \n",
       "15037           NaN           NaN  \n",
       "15994           NaN           NaN  \n",
       "9187            NaN           NaN  \n",
       "12488           NaN           NaN  \n",
       "14506           NaN           NaN  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.nlargest(5, ['posemo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-seeking",
   "metadata": {},
   "source": [
    "The 5 tweets from the most recently collected data that have the highest scores in terms of positive emotions. However, we noticed that even tweets that have a positive sentiment initially can that the overall message is negative. This is why the other metrics are utilized alongside. For comparison, here is the top 5 most negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "promotional-equivalent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12848</th>\n",
       "      <td>32430</td>\n",
       "      <td>1.363322e+18</td>\n",
       "      <td>BIG_B00B$</td>\n",
       "      <td>WEAR A FUCKING MASK YOU STUPID FUCK</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 02:59:35</td>\n",
       "      <td>7</td>\n",
       "      <td>93.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>61597</td>\n",
       "      <td>1.363367e+18</td>\n",
       "      <td>Sassy | BLM</td>\n",
       "      <td>Goodnight. Fuck racists. Fuck Ted Cuntface Cru...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 05:58:21</td>\n",
       "      <td>11</td>\n",
       "      <td>98.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>1.360839e+18</td>\n",
       "      <td>calledryan</td>\n",
       "      <td>copernicus was wrong</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-14 06:30:15</td>\n",
       "      <td>3</td>\n",
       "      <td>18.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>21874</td>\n",
       "      <td>1.363307e+18</td>\n",
       "      <td>KingOfSoup ❼</td>\n",
       "      <td>My mask ugly</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 01:57:06</td>\n",
       "      <td>3</td>\n",
       "      <td>18.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>26761</td>\n",
       "      <td>1.363317e+18</td>\n",
       "      <td>President Dr.Jillian(MAGA Bean)🇺🇸</td>\n",
       "      <td>America is full of fools. Weak mask wearing fo...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-02-21 02:39:01</td>\n",
       "      <td>9</td>\n",
       "      <td>93.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       standard            id                           username  \\\n",
       "12848     32430  1.363322e+18                          BIG_B00B$   \n",
       "20535     61597  1.363367e+18                        Sassy | BLM   \n",
       "235         235  1.360839e+18                         calledryan   \n",
       "9540      21874  1.363307e+18                       KingOfSoup ❼   \n",
       "10850     26761  1.363317e+18  President Dr.Jillian(MAGA Bean)🇺🇸   \n",
       "\n",
       "                                                    text  \\\n",
       "12848                WEAR A FUCKING MASK YOU STUPID FUCK   \n",
       "20535  Goodnight. Fuck racists. Fuck Ted Cuntface Cru...   \n",
       "235                                 copernicus was wrong   \n",
       "9540                                        My mask ugly   \n",
       "10850  America is full of fools. Weak mask wearing fo...   \n",
       "\n",
       "                                                entities retweet_count  \\\n",
       "12848  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "20535  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "235    {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "9540   {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "10850  {'hashtags': [], 'symbols': [], 'user_mentions...             1   \n",
       "\n",
       "      favorite_count           created_at  WC Analytic  ... Quote Apostro  \\\n",
       "12848              0  2021-02-21 02:59:35   7    93.26  ...   0.0     0.0   \n",
       "20535              0  2021-02-21 05:58:21  11    98.34  ...   0.0     0.0   \n",
       "235                0  2021-02-14 06:30:15   3    18.82  ...   0.0     0.0   \n",
       "9540               0  2021-02-21 01:57:06   3    18.82  ...   0.0     0.0   \n",
       "10850              6  2021-02-21 02:39:01   9    93.26  ...   0.0     0.0   \n",
       "\n",
       "      Parenth OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
       "12848     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "20535     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "235       0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "9540      0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "10850     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       Unnamed: 105  Unnamed: 106  \n",
       "12848           NaN           NaN  \n",
       "20535           NaN           NaN  \n",
       "235             NaN           NaN  \n",
       "9540            NaN           NaN  \n",
       "10850           NaN           NaN  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.nlargest(5, ['negemo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-continent",
   "metadata": {},
   "source": [
    "A similar situation happens. We can see that some of these tweets have negative emotions over the fact not enough people are wearing masks while others have negative emotions *because* of masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-absence",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "After collecting and processing the actual data, some thoughts have come up. \n",
    "1. How will we define overall positive sentiment a tweet has against protective public initiatives such as masks, social distancing, vaccines, and so on? It's shown that a tweet can be considered extremely negative but due to not enough people following those public health guidelines. Negativity or positivity of a message doesn't equate to their own feelings about those topics. This will primarily be looking at how LIWC will define sentiment - perhaps combining with either TextBlob or Vader.\n",
    "\n",
    "2. Sizing DOWN our data. Since we're doing a trend over nearly the entirety of the pandemic, we need to be able to define how much data we'll be collecting from each day/week/month/etc. To put things in perspective, over 1 billion tweets have been collected based on keywords relating to the COVID-19 Pandemic. Size, computational, and time-wise it's not feasible to process over 1 billion. Currently, we're thinking about roughly 3000-5000 tweets per week in 2020. \n",
    "\n",
    "3. Explore with clustering on the data. Possibly utilizing Expectation Maximization algorithm to confidently define how many clusters there are. Then within each cluster, gain a sense of what the group represents looking at the matching keywords and LIWC scores.\n",
    "\n",
    "4. For tweets in 2020, must divide up the larger datasets based on the keywords so we can then cross compare the various keywords/topics over time. We can then start to compare recent data about specific keywords with data in 2020 about the same keywords, then do this for the entire time range we have.\n",
    "\n",
    "### Concerns & Notes\n",
    "1. We have noticed a few things after collecting data. Tweets, by default, are limited to 140 characters by default but can get up to 280 by setting \"tweet_mode\" to \"extended\". Since this affects rate limit and the average tweet length is around 30 characters, we've decided not to. \n",
    "2. Some of the tweets may be in different languages. While primarily querying for U.S, not everyone's primary language in the U.S is English; and consequently, they speak, read, interact, etc. in their most comfortabel dialect. We believe this actually won't be a problem due to the small number of these kinds of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-information",
   "metadata": {},
   "source": [
    "## Credit Listing\n",
    "\n",
    "**Lipsa Jena**\n",
    "* Code for scraping current data (2021)\n",
    "* Collected tweets relating to vaccines (Eg. keywords = vaccines, covid vaccine, moderna, pfizer, etc.)\n",
    "\n",
    "**Ji Kang**\n",
    "* Collected past twitter using the IEEE and Kaggle tweet-id datasets\n",
    "* Ran data through LIWC to generate LIWC metrics.\n",
    "\n",
    "**Yuanfeng Li**\n",
    "* Code for scraping current data (2021)\n",
    "* Collected tweets relating to masks (Eg. keywords = wearmask, wearamask, masks4all, n95, respirator, etc.)\n",
    "\n",
    "**Yu Ling**\n",
    "* Code for scraping current data (2021)\n",
    "* Collected tweets relating to preventative measures (Eg. keywords = social distancing, self isolation, quarantine, socialdistancing now, etc.)\n",
    "\n",
    "\n",
    "Note: For the submission, we've included the following files: \n",
    "Files prepended by LIWC2015 denote files that have already been processed and enriched with the LIWC 2015 dictionary model metrics.\n",
    "1. files in /sample_data folder are sized down versions of the tweet IDs both Kaggle and IEEE gives. Named accordingly.\n",
    "2. files in /scraped_Data folder contain samples of collected tweets from 2020 using tweet id as well as recent tweets collected specific keywords. march_31_2020 are tweets collected using tweet id for that day based on all the keywords. 'data_social_distancing' is **recent** (Past week) data collected for keywords related to social distancing. Lastly, 'masks' is **recent** data collected for keywords relating to masks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
