{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "further-chemistry",
   "metadata": {},
   "source": [
    "# Utilizing Twitter to Measure the Public's Sentiment on the Pandemic & Its Relevant Protective Measures Over Time\n",
    "**Team 7:** Insert Name Here \n",
    "\n",
    "**Members:** Lipsa J., Ji K., Yuanfeng L., Yu L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-moscow",
   "metadata": {},
   "source": [
    "### Background and Motivation\n",
    "The pandemic has uprooted the lives of every single person in the world. While it began as a minor inconvenience to many people, the harsh reality and severity of the virus were soon realized. In the beginning of enforcing protective measures to protect the public, many people's opinions on the virus, protective rules & procedures, and other topics relating to the pandemic have changed and continually do into 2021. \n",
    "\n",
    "With such a slow response to protective measures in the U.S compared to other countries globally, we wanted to find out the public's stance on the matter over the period of nearly the entire pandemic. \n",
    "\n",
    "With this in mind, we want to record and analyze these trends by looking at the metrics such as sentiment, LIWC metrics, and possibly more as we make further discoveries.\n",
    "\n",
    "Utilizing Twitter, an online social media platform for sharing content and microblogging, we'll be analyzing \"tweets\" (publically posted messages) from everyday people about how they feel about the pandemic. This procedure will be run on data from January 22 2020, all the way to the most current data being available at this current time of the project (February and March of 2021). We believe this approach will work compared to traditional forms of collecting data on the topic. Twitter specifically has proven itself to be accurate, quick, and better reflect the perspectives of the everyday person since they're the ones whose data we're processing.\n",
    "\n",
    "We've outlined objectives & research questions we hope to answer through this approach.\n",
    "* **Goal 1**: Find out how many tweets sentiments changed on the regulation or rules about wearing a mask or taking a vaccine for the the year 2020 and current months in 2021 (January - March)\n",
    "* **Goal 2**: Find out the sentiment of tweets relating to the COVID-19 virus for the year 2020 and the current months in 2021 (January - March)\n",
    "* **Stretch Goal 1**: Find out the sentiments across geographical locations within the U.S about either protective measures (Eg. Wearing a mask) and the taking the vaccine. It's been shown throughout various news outlets and social media that different areas in the U.S have had varying responses to these rules. If time & resources allow, we want to run the research experiment at a lower level - focusing on specific areas in the U.S - Perhaps areas with the lowest cases per capita vs. moderate vs. high. \n",
    "* **Stretch goal 2**: Relate our findings to how misinformation & fake news on Twitter changed before and after the election; as well as its possible consequences on the public's sentiment on the topic of COVID-19 and its related topics (Eg. vaccines, lockdown, social distancing).\n",
    "\n",
    "Through our efforts, we hope to be able to answer or at least find insight into the following questions as well:\n",
    "* Have specific events affected the public's stance on the pandemic? These could be the presidential election, the presidential candidates debate as well as the vice-presidents debate, Trump getting diagnosed and hospital stayed, and so on. \n",
    "* How have different cities, counties, and states efficacy in containing the virus relate to the public sentiment from the people there?\n",
    "* Is there a positive trend or at least an upward direction for sentiment on social media comparing pre-election to post-election? \n",
    "* What are the trends in Democratic-heavy vs. Republican-heavy vs. Well-mixed (Eg. close to 50/50 Democrat to Republican) States in terms of the pandemic and protective measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-bunch",
   "metadata": {},
   "source": [
    "### Our Current Approach\n",
    "Currently, we're utilizing Twitter for our data collection. Initially, we stated that we'd be utilizing Reddit as well but the caveats of a public forum platform is that it's heavily moderated. With the pandemic being a global crisis, Reddit has emphasized initiatives to remove and censor posts that may be incediary, controversial, promote misinformation and so on. While these things are negative in the grand scheme of society, we actually want to collect this kind of data as well since it shows a sub-population with different views. There is a promising subreddit about Parler, the right-wing social media platform, in which users post the most outrageous or controversial posts they see on the platform to re-post and discuss on reddit. This seemed promising at first but the format mirrors satire and other users have cherry-picked those specific posts and typically upload them as images. It wouldn't accurately reflect the Parler population on specific topics, would be difficult to parse for specific keywords, and may not produce enough data.\n",
    "\n",
    "We're utilizing Tweepy which is Twitter's API wrapper for Python. It's extremely easy to utilize but one of its caveats is that it will only look at the past week to pull data; which makes sense since many people actually use real-time data for analysis. To get around this issue, we relied on Kaggle and IEEE.\n",
    "Both of them have been data mining the ID number of tweets with keywords relating to the pandemic since near the beginning of 2020. These keywords include identifiers such as \"n95\", \"ppe\", \"washyourhands\", \"stayathome\", \"selfisolating\", \"social distancing\", \"covid-19\", and so on. \n",
    "\n",
    "Utilizing Tweepy and Python, we iterate through these tweet id values to pull the actual tweet status object from Twitter. From there, we extract the following information: \n",
    "* id: ID number of the tweet\n",
    "* username: Username of the person who posted the tweet\n",
    "* text: The literal text content of the tweet\n",
    "* entities: Hashtags the tweet had\n",
    "* retweet_count: Number of times the tweet had been retweeted\n",
    "* favorite_count: Number of times the tweet had been favorited\n",
    "* created_at: Time the tweet had been posted\n",
    "\n",
    "We're collecting our own data currently with the same parameters and keywords. Taking these datasets, in a csv format, we're running each through LIWC and looking at the following metrics: \n",
    "* Summary variables: Analytical thinking, clout, authentic, and emotional tone\n",
    "* Affect words: Positive emotions, negative emotions, anxiety, anger, sadness\n",
    "* Social words: Family, friends, female referents, male referents\n",
    "* Cognitive Processes: Insight\n",
    "* Biological processes: Body, health/illness\n",
    "* Personal concerns: Work, leisure, money\n",
    "* Informal speech: Swear words\n",
    "\n",
    "While the biggest contributors will be relating to authenticity, emotions (emotional tone & positive/negative emotions), we believe the other attributes will aid answering in our research questions and stretch goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-gospel",
   "metadata": {},
   "source": [
    "## Collecting Twitter Data From the Entirety of 2020\n",
    "As mentioned previously, since we can only directly scrape tweets for the past week, we utilize Kaggle's dataset which is found at https://www.kaggle.com/lopezbec/covid19-tweets-dataset.\n",
    "\n",
    "Additionally, the IEEE have published a similar dataset with a wider range of keywords which can be found at https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset. \n",
    "\n",
    "These files contain minimal data in order to save space. Kaggle's has just the tweet ids in a list-like structure, while the IEEE has a similar format but in pairs of tweet id and a sentiment score calculated for the content of the tweet. \n",
    "\n",
    "Here are a sample of what the Kaggle files look like. We'll be including sized down versions of them in our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valid-listening",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in sample: 31293\n",
      "Sample of the Tweet IDS: [1236295217130717186 1236295217134874624 1236295217231187971 ...\n",
      " 1236440755394301952 1236440756002283522 1236440756392325120]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "kaggle_sample = open('./sample_data/sample_kaggle.txt', 'r')\n",
    "#removes beginning and ending '[' and  ']'\n",
    "content = kaggle_sample.read()[1:-1] \n",
    "#Delimit on comma, convert to int, store into a numpy array for parsing\n",
    "ids = np.fromstring(content, dtype=int, sep= ',')\n",
    "print(\"Number of tweets in sample: {}\".format(len(ids)))\n",
    "print(\"Sample of the Tweet IDS: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-commercial",
   "metadata": {},
   "source": [
    "As mentioned, to save space Kaggle stores these tweet ids as a plaintext, in a list like structure... Eg. [id1, id2, id3, ...., idn]\n",
    "\n",
    "On the otherhand, IEEE provides theirs as a csv format or a zipped file containing a csv file (to save space) and theirs looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "determined-weapon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240727808080412673</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240727808005079041</td>\n",
       "      <td>0.116071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240727808340414464</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240727808629813248</td>\n",
       "      <td>-0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240727808617230336</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  sentiment\n",
       "0  1240727808080412673   0.357143\n",
       "1  1240727808005079041   0.116071\n",
       "2  1240727808340414464  -0.050000\n",
       "3  1240727808629813248  -0.714286\n",
       "4  1240727808617230336   0.700000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ieee_sample = pd.read_csv(\"./sample_data/sample_ieee.csv\", header=None)\n",
    "ieee_sample.columns = ['tweet_id', 'sentiment']\n",
    "ieee_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-cartoon",
   "metadata": {},
   "source": [
    "The IEEE data is much easier to work with since we can extract the tweet_id values directly by extracting the column. However, both are eventually in the same format which lets us run our data collector.\n",
    "It should be noted that, unlike the Tweepy API's 7 day period that it can return data for, the get_status() method for getting individual tweets given a tweet id does each call one by one. This takes much longer time but allows us to still get 15,000 records under the rate limit which means we collect data as fast as Tweepy will let us regardless.\n",
    "\n",
    "## Processing the Tweet IDs\n",
    "Due to the organizational structure both IEEE and Kaggle have (Eg. each day have their own files, files are grouped by month, etc.), we followed a similar approach. Below is code taken from a previous personal project Ji-Hoon has done just to iterate through directories. The directory navigation portion isn't entirely important but for breadth and covering bases, we've included this portion if you want to run the file as well.\n",
    "\n",
    "To perserve the name and directory structure each sub-directory has, it is a recursive method that holds the absolute path of the files (So Python can read in the file's contents) while also preserving the subdirectory path so we can name each file accordingly and know which dataset it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-chapter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "\n",
    "max_num_records = 5000\n",
    "\n",
    "\"\"\"\n",
    "Methodology we covered in class to just load the twitter credentials into appropriate objects.\n",
    "This assumes a file with your Twitter developer credentials are in a file named 'twitter.json' \n",
    "and is in the same directory as the program when it's being run.\n",
    "\"\"\"\n",
    "def load_keys(key_file):\n",
    "    with open(key_file) as f:\n",
    "        key_dict = json.load(f)\n",
    "    return key_dict['api_key'], key_dict['api_secret'], key_dict['token'], key_dict['token_secret']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Recursive method to navigate through many directories.\n",
    "\"\"\"\n",
    "def iterate_files(path, subdir):\n",
    "    KEY_FILE = \"./twitter.json\" # Twitter credentials. See def load_keys(key_file):\n",
    "    api_key, api_secret, token, token_secret = load_keys(KEY_FILE)\n",
    "    auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "    auth.set_access_token(token, token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    # File recursion portion. Not pertinent to the actual data collection.\n",
    "    for filename in os.listdir(path):\n",
    "        filePath = path + \"/\" + filename\n",
    "        # If folder, recursively call.\n",
    "        if (os.path.isdir(filePath)):\n",
    "            tempSubdir = \"\"\n",
    "            if subdir: tempSubdir = subdir + \"/\" + filename\n",
    "            else: tempSubdir = filename\n",
    "            iterate_files(filePath, tempSubdir)\n",
    "        # Otherwise, process the file.\n",
    "        else:\n",
    "            filekey = subdir\n",
    "            if subdir: file = subdir + \"/\" + filename\n",
    "            else: file = filename\n",
    "            tweet_content = defaultdict(list)\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO: Refactor to detect .txt and .csv to know which dataset it came from. \n",
    "            \"\"\"\n",
    "            # So it doesn't read itself or the credentials file\n",
    "            if filename not in ['process_ieee.py', 'process_tweets.py', 'twitter.json']: \n",
    "                # NOTE: This version of the program is assuming .csv files -- IEEE data. \n",
    "                # Samples 5000 records from the data set, takes only the column values, then ravels into a np array\n",
    "                tweet_ids = pd.read_csv(filePath).sample(n=max_num_records).iloc[:,0].values.ravel()\n",
    "                \n",
    "                # Iterates through each of the tweet ids we sampled.\n",
    "                for id in tweet_ids: \n",
    "                    \"\"\"\n",
    "                    Must be in a try-catch structure. If a twitter user is banned or suspended, the tweet_id\n",
    "                    refers to data that doesn't exist. The Tweepy api will return a 400-level HTTP status code\n",
    "                    due to the resource not being found - which is considered an exception.\n",
    "                    \"\"\"\n",
    "                    try: \n",
    "                        print(num_records) # Debugging. Just to see that the program wasn't stalling.\n",
    "                        tweet = api.get_status(id) #returns status object\n",
    "                    except tweepy.RateLimitError:\n",
    "                        print(\"Rate Limit hit. Sleeping for 15 minutes.\")\n",
    "                        time.sleep(900)\n",
    "                        print(\"Resuming...\\n\")\n",
    "                        continue\n",
    "                    except Exception as e: # It will throw an exception if twitter user has actually been suspended\n",
    "                        continue\n",
    "                    if tweet is None:\n",
    "                        print(\"Should never be reached. If seen, something went wrong.\")\n",
    "                    \n",
    "                    # Features we're extracting.\n",
    "                    tweet_content['id'].append(tweet.id)\n",
    "                    tweet_content['username'].append(tweet.user.name)\n",
    "                    tweet_content['text'].append(tweet.text)\n",
    "                    tweet_content['entities'].append(tweet.entities)\n",
    "                    tweet_content['retweet_count'].append(tweet.retweet_count)\n",
    "                    tweet_content['favorite_count'].append(tweet.favorite_count)\n",
    "                    tweet_content['created_at'].append(tweet.created_at)\n",
    "                result_filename = './scraped_data/' + filename\n",
    "                \"\"\"\n",
    "                We control how many records we want from each day. It'll either run through the entire file\n",
    "                or run based on how many records we want sampled.\n",
    "                \"\"\"\n",
    "                pd.DataFrame(tweet_content).to_csv(result_filename)\n",
    "                print(\"Done processing: {}\".format(result_filename))\n",
    "\n",
    "\n",
    "def get_path():\n",
    "    iterate_files(os.getcwd(), \"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-fluid",
   "metadata": {},
   "source": [
    "## Data Collection For Recent Data in 2021\n",
    "We relied on those datasets to help supplement the earlier twitter data we can't directly retrieve. However, for the current data, we're querying and data mining with a similar approach. \n",
    "\n",
    "We have taken inspiration from the IEEE Coronavirus (COVID-19) Tweets Dataset, which can be found at https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset. They have collected tweets relating to a large set of keywords since the very beginning of the pandemic and continually do so. We have taken a scaled down version and taken specific keywords from their larger set - which can be found at https://rlamsal.com.np/keywords.tsv. Note: The link will start the download of a tab-separated file with the keywords but is small in terms of memory size. Just a warning.\n",
    "\n",
    "The approach for collecting recent data has stayed essentially the same as we did in our Lab 3 for the introduction to Tweepy. One difference is that, since we're all constrained with the rate limit, we've separated the main topics to be scraping for. The keywords each person utilized to scrape will be shown at the credits/work distribution section at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: This is assuming the credentials were loaded properly like in the previous code snippets.\n",
    "\"\"\"\n",
    "csvFile = open('vaccinetweets.csv', 'a')\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "# search_term may differ\n",
    "search_term = \"Vaccine OR vaccine against coronavirus OR Vaccines work OR Covid Vaccine OR #vaccine2021\"\n",
    "new_search = search_term \n",
    "\n",
    "for page in tweepy.Cursor(api.search, wait_on_rate_limit=True, q = new_search, lang=\"en\",tweet_mode=\"extended\",since=\"2021-02-15\").pages():\n",
    "    for status in page:\n",
    "        csvWriter.writerow([status.id ,status.user.name , status.full_text,status.entities , status.retweet_count, status.favorite_count, status.created_at])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-judges",
   "metadata": {},
   "source": [
    "## Processing the Data\n",
    "We collected all the data in the same fashion, ran the text fields through LIWC, and separated files to organize based on the time period they represent. We ran these files through LIWC's program which outputs a copy of the same file but with new columns pertaining to the metrics and corresponding values LIWC has produced.\n",
    "\n",
    "A snippet of the outputted file can be shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "racial-comparative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   standard            id            username  \\\n",
      "0         0  1.363275e+18        Fire Is Born   \n",
      "1         1  1.363263e+18   MyFrenchDietitian   \n",
      "2         2  1.363260e+18  healingcolorsmusic   \n",
      "3         3  1.363260e+18  healingcolorsmusic   \n",
      "4         4  1.363260e+18  healingcolorsmusic   \n",
      "\n",
      "                                                text  \\\n",
      "0  @iamungit I've been in one of them in San Fran...   \n",
      "1  Enjoy the #weekend, go #outdoor, reconnect wit...   \n",
      "2  #healingcolorsmusic #art #music ...there is #S...   \n",
      "3  #healingcolorsmusic #art #music ...there is #S...   \n",
      "4  #healingcolorsmusic #art #music ...there is #S...   \n",
      "\n",
      "                                            entities retweet_count  \\\n",
      "0  {'hashtags': [{'text': 'WearMask', 'indices': ...             1   \n",
      "1  {'hashtags': [{'text': 'weekend', 'indices': [...             0   \n",
      "2  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "3  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "4  {'hashtags': [{'text': 'healingcolorsmusic', '...             0   \n",
      "\n",
      "  favorite_count           created_at  WC Analytic  ... Quote Apostro Parenth  \\\n",
      "0              1  2021-02-20 23:52:00  25    93.26  ...   0.0     4.0     0.0   \n",
      "1              1  2021-02-20 23:01:59  24    93.26  ...   0.0     0.0     0.0   \n",
      "2              1  2021-02-20 22:52:27  19    93.26  ...   0.0     0.0     0.0   \n",
      "3              1  2021-02-20 22:50:59  19    93.26  ...   0.0     0.0     0.0   \n",
      "4              1  2021-02-20 22:49:58  19    93.26  ...   0.0     0.0     0.0   \n",
      "\n",
      "  OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
      "0  16.00           NaN           NaN           NaN           NaN   \n",
      "1  29.17           NaN           NaN           NaN           NaN   \n",
      "2  52.63           NaN           NaN           NaN           NaN   \n",
      "3  52.63           NaN           NaN           NaN           NaN   \n",
      "4  52.63           NaN           NaN           NaN           NaN   \n",
      "\n",
      "   Unnamed: 105  Unnamed: 106  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 107 columns]\n",
      "Column Names: ['standard', 'id', 'username', 'text', 'entities', 'retweet_count', 'favorite_count', 'created_at', 'WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP', 'Unnamed: 101', 'Unnamed: 102', 'Unnamed: 103', 'Unnamed: 104', 'Unnamed: 105', 'Unnamed: 106']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihk/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (10,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_csv('LIWC2015_feb.csv')\n",
    "print(sample.head())\n",
    "\n",
    "print(\"Column Names: {}\".format(list(sample.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-canada",
   "metadata": {},
   "source": [
    "We have noticed some problems across different operating systems for handling csv files. We initially ran into problems while sharing datasets with each other across Debian, Windows, and Mac and have resolved most of them since. One example is that some empty columns will show themselves as \"Unnamed\" columns with empty or NaN values in them. We ignore these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dress-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>40342</td>\n",
       "      <td>1.363335e+18</td>\n",
       "      <td>King Jamison Fawkes ♚</td>\n",
       "      <td>@WildHogPower \"WELL WELL WELL WELL WELL WELL W...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 03:50:29</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>43474</td>\n",
       "      <td>1.363335e+18</td>\n",
       "      <td>King Jamison Fawkes ♚</td>\n",
       "      <td>@WildHogPower \"WELL WELL WELL WELL WELL WELL W...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-21 03:50:29</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9187</th>\n",
       "      <td>21521</td>\n",
       "      <td>1.363309e+18</td>\n",
       "      <td>FA_eye(formally Accureye) #CyberPunk2077</td>\n",
       "      <td>@TheSphereHunter Nice love it great mask</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 02:05:49</td>\n",
       "      <td>6</td>\n",
       "      <td>62.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12488</th>\n",
       "      <td>32070</td>\n",
       "      <td>1.363325e+18</td>\n",
       "      <td>TEA POt</td>\n",
       "      <td>@FailedSoul_ *winning laughs* pretty impressiv...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 03:10:45</td>\n",
       "      <td>8</td>\n",
       "      <td>72.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14506</th>\n",
       "      <td>38373</td>\n",
       "      <td>1.363325e+18</td>\n",
       "      <td>TEA POt</td>\n",
       "      <td>@FailedSoul_ *winning laughs* pretty impressiv...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-21 03:10:45</td>\n",
       "      <td>8</td>\n",
       "      <td>72.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       standard            id                                  username  \\\n",
       "15037     40342  1.363335e+18                     King Jamison Fawkes ♚   \n",
       "15994     43474  1.363335e+18                     King Jamison Fawkes ♚   \n",
       "9187      21521  1.363309e+18  FA_eye(formally Accureye) #CyberPunk2077   \n",
       "12488     32070  1.363325e+18                                   TEA POt   \n",
       "14506     38373  1.363325e+18                                   TEA POt   \n",
       "\n",
       "                                                    text  \\\n",
       "15037  @WildHogPower \"WELL WELL WELL WELL WELL WELL W...   \n",
       "15994  @WildHogPower \"WELL WELL WELL WELL WELL WELL W...   \n",
       "9187            @TheSphereHunter Nice love it great mask   \n",
       "12488  @FailedSoul_ *winning laughs* pretty impressiv...   \n",
       "14506  @FailedSoul_ *winning laughs* pretty impressiv...   \n",
       "\n",
       "                                                entities retweet_count  \\\n",
       "15037  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "15994  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "9187   {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "12488  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "14506  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "\n",
       "      favorite_count           created_at  WC Analytic  ... Quote Apostro  \\\n",
       "15037              0  2021-02-21 03:50:29  26        1  ...  3.85     0.0   \n",
       "15994              1  2021-02-21 03:50:29  26        1  ...  3.85     0.0   \n",
       "9187               0  2021-02-21 02:05:49   6    62.04  ...  0.00     0.0   \n",
       "12488              0  2021-02-21 03:10:45   8    72.69  ...  0.00     0.0   \n",
       "14506              1  2021-02-21 03:10:45   8    72.69  ...  0.00     0.0   \n",
       "\n",
       "      Parenth OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
       "15037     0.0  11.54           NaN           NaN           NaN           NaN   \n",
       "15994     0.0  11.54           NaN           NaN           NaN           NaN   \n",
       "9187      0.0  16.67           NaN           NaN           NaN           NaN   \n",
       "12488     0.0  50.00           NaN           NaN           NaN           NaN   \n",
       "14506     0.0  50.00           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       Unnamed: 105  Unnamed: 106  \n",
       "15037           NaN           NaN  \n",
       "15994           NaN           NaN  \n",
       "9187            NaN           NaN  \n",
       "12488           NaN           NaN  \n",
       "14506           NaN           NaN  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.nlargest(5, ['posemo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-seeking",
   "metadata": {},
   "source": [
    "The 5 tweets from the most recently collected data that have the highest scores in terms of positive emotions. However, we noticed that even tweets that have a positive sentiment initially can that the overall message is negative. This is why the other metrics are utilized alongside. For comparison, here is the top 5 most negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "promotional-equivalent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12848</th>\n",
       "      <td>32430</td>\n",
       "      <td>1.363322e+18</td>\n",
       "      <td>BIG_B00B$</td>\n",
       "      <td>WEAR A FUCKING MASK YOU STUPID FUCK</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 02:59:35</td>\n",
       "      <td>7</td>\n",
       "      <td>93.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>61597</td>\n",
       "      <td>1.363367e+18</td>\n",
       "      <td>Sassy | BLM</td>\n",
       "      <td>Goodnight. Fuck racists. Fuck Ted Cuntface Cru...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 05:58:21</td>\n",
       "      <td>11</td>\n",
       "      <td>98.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>1.360839e+18</td>\n",
       "      <td>calledryan</td>\n",
       "      <td>copernicus was wrong</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-14 06:30:15</td>\n",
       "      <td>3</td>\n",
       "      <td>18.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>21874</td>\n",
       "      <td>1.363307e+18</td>\n",
       "      <td>KingOfSoup ❼</td>\n",
       "      <td>My mask ugly</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-21 01:57:06</td>\n",
       "      <td>3</td>\n",
       "      <td>18.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>26761</td>\n",
       "      <td>1.363317e+18</td>\n",
       "      <td>President Dr.Jillian(MAGA Bean)🇺🇸</td>\n",
       "      <td>America is full of fools. Weak mask wearing fo...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-02-21 02:39:01</td>\n",
       "      <td>9</td>\n",
       "      <td>93.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       standard            id                           username  \\\n",
       "12848     32430  1.363322e+18                          BIG_B00B$   \n",
       "20535     61597  1.363367e+18                        Sassy | BLM   \n",
       "235         235  1.360839e+18                         calledryan   \n",
       "9540      21874  1.363307e+18                       KingOfSoup ❼   \n",
       "10850     26761  1.363317e+18  President Dr.Jillian(MAGA Bean)🇺🇸   \n",
       "\n",
       "                                                    text  \\\n",
       "12848                WEAR A FUCKING MASK YOU STUPID FUCK   \n",
       "20535  Goodnight. Fuck racists. Fuck Ted Cuntface Cru...   \n",
       "235                                 copernicus was wrong   \n",
       "9540                                        My mask ugly   \n",
       "10850  America is full of fools. Weak mask wearing fo...   \n",
       "\n",
       "                                                entities retweet_count  \\\n",
       "12848  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "20535  {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "235    {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "9540   {'hashtags': [], 'symbols': [], 'user_mentions...             0   \n",
       "10850  {'hashtags': [], 'symbols': [], 'user_mentions...             1   \n",
       "\n",
       "      favorite_count           created_at  WC Analytic  ... Quote Apostro  \\\n",
       "12848              0  2021-02-21 02:59:35   7    93.26  ...   0.0     0.0   \n",
       "20535              0  2021-02-21 05:58:21  11    98.34  ...   0.0     0.0   \n",
       "235                0  2021-02-14 06:30:15   3    18.82  ...   0.0     0.0   \n",
       "9540               0  2021-02-21 01:57:06   3    18.82  ...   0.0     0.0   \n",
       "10850              6  2021-02-21 02:39:01   9    93.26  ...   0.0     0.0   \n",
       "\n",
       "      Parenth OtherP  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
       "12848     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "20535     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "235       0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "9540      0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "10850     0.0    0.0           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       Unnamed: 105  Unnamed: 106  \n",
       "12848           NaN           NaN  \n",
       "20535           NaN           NaN  \n",
       "235             NaN           NaN  \n",
       "9540            NaN           NaN  \n",
       "10850           NaN           NaN  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.nlargest(5, ['negemo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-continent",
   "metadata": {},
   "source": [
    "A similar situation happens. We can see that some of these tweets have negative emotions over the fact not enough people are wearing masks while others have negative emotions *because* of masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-helmet",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "After collecting and processing the actual data, some thoughts have come up. \n",
    "1. How will we define overall positive sentiment a tweet has against protective public initiatives such as masks, social distancing, vaccines, and so on? It's shown that a tweet can be considered extremely negative but due to not enough people following those public health guidelines. Negativity or positivity of a message doesn't equate to their own feelings about those topics. This will primarily be looking at how LIWC will define sentiment - perhaps combining with either TextBlob or Vader.\n",
    "\n",
    "2. Sizing DOWN our data. Since we're doing a trend over nearly the entirety of the pandemic, we need to be able to define how much data we'll be collecting from each day/week/month/etc. To put things in perspective, over 1 billion tweets have been collected based on keywords relating to the COVID-19 Pandemic. Size, computational, and time-wise it's not feasible to process over 1 billion. Currently, we're thinking about roughly 3000-5000 tweets per week. \n",
    "\n",
    "3. Explore with clustering on the data. Possibly utilizing Expectation Maximization algorithm to confidently define how many clusters there are. Then within each cluster, gain a sense of what the group represents looking at the matching keywords and LIWC scores.\n",
    "\n",
    "### Concerns & Notes\n",
    "1. We have noticed a few things after collecting data. Tweets, by default, are limited to 140 characters by default but can get up to 280 by setting \"tweet_mode\" to \"extended\". Since this affects rate limit and the average tweet length is around 30 characters, we've decided not to. \n",
    "2. Some of the tweets may be in different languages. While primarily querying for U.S, not everyone's primary language in the U.S is English; and consequently, they speak, read, interact, etc. in their most comfortabel dialect. We believe this actually won't be a problem due to the small number of these kinds of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-information",
   "metadata": {},
   "source": [
    "## Credit Listing\n",
    "\n",
    "**Lipsa Jena**\n",
    "* asd\n",
    "\n",
    "**Ji Kang**\n",
    "* Collected past twitter using the IEEE and Kaggle tweet-id datasets\n",
    "* Ran data through LIWC to generate LIWC metrics.\n",
    "\n",
    "**Yuanfeng Li**\n",
    "* asd\n",
    "\n",
    "**Yu Ling**\n",
    "* asd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
